{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-based Neural Machine Translation\n",
    "\n",
    "**Reference:** Luong, Thang, Hieu Pham and Christopher D. Manning. \"Effective Approaches to Attention-based Neural Machine Translation.\" In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1412-1421. 2015.\n",
    "\n",
    "* https://www.aclweb.org/anthology/D15-1166/ (main paper reference)\n",
    "* https://arxiv.org/abs/1508.04025 (alternative paper url)\n",
    "* https://github.com/tensorflow/nmt (main code reference)\n",
    "* https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention (alternative code reference)\n",
    "* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py:2449,2103 (attention implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, Random, CuArrays, IterTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and data from previous projects\n",
    "\n",
    "Please copy or include the following types and related functions from previous projects:\n",
    "`Vocab`, `TextReader`, `MTData`, `Embed`, `Linear`, `mask!`, `loss`, `int2str`,\n",
    "`bleu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "function Vocab(file::String; tokenizer=split, vocabsize=Inf, mincount=1, unk=\"<unk>\", eos=\"<s>\")\n",
    "    word_count = Dict{String,Int}()\n",
    "    w2i = Dict{String,Int}()\n",
    "    i2w = Vector{String}()\n",
    "    int_eos = get!(w2i, eos, 1+length(w2i))\n",
    "    int_unk = get!(w2i, unk, 1+length(w2i))\n",
    "    for line in eachline(file)\n",
    "        line = tokenizer(line)\n",
    "        for word in line\n",
    "            if haskey(word_count, word)\n",
    "                word_count[word] += 1\n",
    "            else\n",
    "                word_count[word] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    word_count = collect(word_count)\n",
    "    sort!(word_count, rev=true, by=x->x[2])\n",
    "    # constructing w2i\n",
    "    for pair in word_count\n",
    "        if pair[2] >= mincount\n",
    "            get!(w2i, pair[1], 1+length(w2i))\n",
    "            if length(w2i) >= vocabsize\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    w2i_array = collect(w2i)\n",
    "    sort!(w2i_array, by=x->x[2])\n",
    "    for pair in w2i_array\n",
    "        push!(i2w, pair[1])\n",
    "    end\n",
    "    return Vocab(w2i, i2w, int_unk, int_eos, tokenizer)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    s == nothing && (s = open(r.file))\n",
    "    if eof(s)\n",
    "        close(s)\n",
    "        return nothing\n",
    "    end\n",
    "    line = readline(s)\n",
    "    line = r.vocab.tokenizer(line)\n",
    "    line_inds = Int[]\n",
    "    for word in line\n",
    "        push!(line_inds, get(r.vocab.w2i, word, r.vocab.unk))\n",
    "    end\n",
    "    return line_inds, s\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Embed; w; end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    return Embed(param(embedsize, vocabsize, atype = Knet.atype()))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    l.w[:,x]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    return Linear(param(outputsize, inputsize, atype = Knet.atype()), param0(outputsize, atype = Knet.atype()))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    l.w * x .+ l.b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask! (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mask!(a,pad)\n",
    "    num_rows, num_cols = size(a)\n",
    "    for row in 1:num_rows\n",
    "        for column in num_cols:-1:1\n",
    "            if a[row, column] != pad || a[row, column-1] != pad\n",
    "                break\n",
    "            else\n",
    "                a[row, column] = 0\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MTData\n",
    "    src::TextReader        # reader for source language data\n",
    "    tgt::TextReader        # reader for target language data\n",
    "    batchsize::Int         # desired batch size\n",
    "    maxlength::Int         # skip if source sentence above maxlength\n",
    "    batchmajor::Bool       # batch dims (B,T) if batchmajor=false (default) or (T,B) if true.\n",
    "    bucketwidth::Int       # batch sentences with length within bucketwidth of each other\n",
    "    buckets::Vector        # sentences collected in separate arrays called buckets for each length range\n",
    "    batchmaker::Function   # function that turns a bucket into a batch.\n",
    "end\n",
    "\n",
    "function MTData(src::TextReader, tgt::TextReader; batchmaker = arraybatch, batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 10, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar length\n",
    "    MTData(src, tgt, batchsize, maxlength, batchmajor, bucketwidth, buckets, batchmaker)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{MTData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{MTData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{MTData}) = NTuple{2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.iterate(d::MTData, state=nothing)\n",
    "    if state == nothing\n",
    "        for i in 1:length(d.buckets)\n",
    "            d.buckets[i] = []\n",
    "        end\n",
    "    end\n",
    "      \n",
    "    while true\n",
    "        if state == nothing\n",
    "            s_data,s_state = iterate(d.src)\n",
    "            t_data,t_state = iterate(d.tgt)\n",
    "        elseif state == \"eof\"\n",
    "             #if there are no more half filled batches, return nothing\n",
    "            for i in 1:length(d.buckets)\n",
    "                if length(d.buckets[i]) > 0\n",
    "                    bucket = d.buckets[i]\n",
    "                    d.buckets[i] = []\n",
    "                    return (d.batchmaker(d,bucket),\"eof\")\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            #no more batches\n",
    "            return nothing\n",
    "        else\n",
    "            src = iterate(d.src,state[1]) \n",
    "            tgt = iterate(d.tgt,state[2])\n",
    "            \n",
    "            #TextReader returns a \"nothing\" value when it finishes, so i need to turn it into a Tuple for \n",
    "            #my code to run as intended\n",
    "            if src == nothing; src = (nothing,nothing);end\n",
    "            if tgt == nothing; tgt = (nothing,nothing);end\n",
    "\n",
    "            s_data,s_state = src\n",
    "            t_data,t_state = tgt\n",
    "        end\n",
    "        \n",
    "        \n",
    "        #if we reached the end of the src or tgt files, return half filled buckets\n",
    "        if (s_state == nothing) | (t_state ==  nothing)\n",
    "            state = \"eof\"\n",
    "            continue\n",
    "        end\n",
    "        \n",
    "        #if source sentence above d.maxlength\n",
    "        if length(s_data) > d.maxlength\n",
    "            state = (s_state,t_state)\n",
    "            continue\n",
    "        #if src sentence larger than a condition specified by Deniz\n",
    "        elseif length(s_data) >=  length(d.buckets)*d.bucketwidth\n",
    "            \n",
    "            push!(d.buckets[length(d.buckets)],(s_data,t_data))\n",
    "            if length(d.buckets[length(d.buckets)]) == d.batchsize\n",
    "                bucket = d.buckets[length(d.buckets)]\n",
    "                d.buckets[length(d.buckets)] = []\n",
    "                return (d.batchmaker(d,bucket),(s_state,t_state))\n",
    "            end\n",
    "        #of if src sentence in range\n",
    "            \n",
    "        else\n",
    "            \n",
    "            for i in 1:length(d.buckets)\n",
    "                if length(s_data) in ((i-1)*d.bucketwidth+1):(i*d.bucketwidth)\n",
    "                    push!(d.buckets[i],(s_data,t_data))\n",
    "                    if length(d.buckets[i]) == d.batchsize\n",
    "                        bucket = d.buckets[i]\n",
    "                        d.buckets[i] = []\n",
    "                        return (d.batchmaker(d,bucket),(s_state,t_state))\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        state = (s_state,t_state)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arraybatch (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function arraybatch(d::MTData, bucket)\n",
    "    x = [] \n",
    "    y = []\n",
    "    \n",
    "    srclength = max([length(pair[1]) for pair in bucket]...)\n",
    "    tgtlength = max([length(pair[2]) for pair in bucket]...)\n",
    "    \n",
    "    for pair in bucket\n",
    "        src_sentence, tgt_sentence = pair\n",
    "        src_sen = []\n",
    "        tgt_sen = []\n",
    "                \n",
    "        tgt_eos = d.tgt.vocab.eos\n",
    "        push!(tgt_sen, tgt_eos)\n",
    "        for w in tgt_sentence\n",
    "            push!(tgt_sen, w) \n",
    "        end\n",
    "        while length(tgt_sen) != (tgtlength + 2)\n",
    "            push!(tgt_sen, tgt_eos)\n",
    "        end\n",
    "        \n",
    "        src_eos = d.src.vocab.eos\n",
    "        eos_num = srclength - length(src_sentence)\n",
    "        i = 0\n",
    "        while i!= eos_num\n",
    "            push!(src_sen, src_eos)\n",
    "            i += 1\n",
    "        end\n",
    "        for w in src_sentence\n",
    "            push!(src_sen, w) \n",
    "        end        \n",
    "        push!(x, src_sen)\n",
    "        push!(y, tgt_sen)\n",
    "    end\n",
    "    \n",
    "    return Matrix(hcat(x...)'), Matrix(hcat(y...)')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int2str (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility to convert int arrays to sentence strings\n",
    "function int2str(y,vocab)\n",
    "    y = vec(y)\n",
    "    ysos = findnext(w->!isequal(w,vocab.eos), y, 1)\n",
    "    ysos == nothing && return \"\"\n",
    "    yeos = something(findnext(isequal(vocab.eos), y, ysos), 1+length(y))\n",
    "    join(vocab.i2w[y[ysos:yeos-1]], \" \")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bleu (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bleu(s2s,d::MTData)\n",
    "    d = MTData(d.src,d.tgt,batchsize=1)\n",
    "    reffile = d.tgt.file\n",
    "    hypfile,hyp = mktemp()\n",
    "    for (x,y) in progress(collect(d))\n",
    "        g = s2s(x)\n",
    "        for i in 1:size(y,1)\n",
    "            println(hyp, int2str(g[i,:], d.tgt.vocab))\n",
    "        end\n",
    "    end\n",
    "    close(hyp)\n",
    "    isfile(\"multi-bleu.perl\") || download(\"https://github.com/moses-smt/mosesdecoder/raw/master/scripts/generic/multi-bleu.perl\", \"multi-bleu.perl\")\n",
    "    run(pipeline(`cat $hypfile`,`perl multi-bleu.perl $reffile`))\n",
    "    return hypfile\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, data; average=true)\n",
    "    loss = 0\n",
    "    count = 0\n",
    "    Σloss = 0\n",
    "    Nloss = 0\n",
    "    for (x, y) in data\n",
    "        loss, count = model(x, y, average=false)\n",
    "        Σloss += loss\n",
    "        Nloss += count\n",
    "    end\n",
    "    \n",
    "    if average\n",
    "        return Σloss/Nloss\n",
    "    else\n",
    "        return Σloss, Nloss\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2S: Sequence to sequence model with attention\n",
    "\n",
    "In this project we will define, train and evaluate a sequence to sequence encoder-decoder\n",
    "model with attention for Turkish-English machine translation. The model has two extra\n",
    "fields compared to `S2S_v1`: the `memory` layer computes keys and values from the encoder,\n",
    "the `attention` layer computes the attention vector for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Memory; w; end\n",
    "\n",
    "struct Attention; wquery; wattn; scale; end\n",
    "\n",
    "struct S2S\n",
    "    srcembed::Embed       # encinput(B,Tx) -> srcembed(Ex,B,Tx)\n",
    "    encoder::RNN          # srcembed(Ex,B,Tx) -> enccell(Dx*H,B,Tx)\n",
    "    memory::Memory        # enccell(Dx*H,B,Tx) -> keys(H,Tx,B), vals(Dx*H,Tx,B)\n",
    "    tgtembed::Embed       # decinput(B,Ty) -> tgtembed(Ey,B,Ty)\n",
    "    decoder::RNN          # tgtembed(Ey,B,Ty) . attnvec(H,B,Ty)[t-1] = (Ey+H,B,Ty) -> deccell(H,B,Ty)\n",
    "    attention::Attention  # deccell(H,B,Ty), keys(H,Tx,B), vals(Dx*H,Tx,B) -> attnvec(H,B,Ty)\n",
    "    projection::Linear    # attnvec(H,B,Ty) -> proj(Vy,B,Ty)\n",
    "    dropout::Real         # dropout probability\n",
    "    srcvocab::Vocab       # source language vocabulary\n",
    "    tgtvocab::Vocab       # target language vocabulary\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Model constructor\n",
    "\n",
    "The `S2S` constructor takes the following arguments:\n",
    "* `hidden`: size of the hidden vectors for both the encoder and the decoder\n",
    "* `srcembsz`, `tgtembsz`: size of the source/target language embedding vectors\n",
    "* `srcvocab`, `tgtvocab`: the source/target language vocabulary\n",
    "* `layers=1`: number of layers\n",
    "* `bidirectional=false`: whether the encoder is bidirectional\n",
    "* `dropout=0`: dropout probability\n",
    "\n",
    "Hints:\n",
    "* You can find the vocabulary size with `length(vocab.i2w)`.\n",
    "* If the encoder is bidirectional `layers` must be even and the encoder should have `layers÷2` layers.\n",
    "* The decoder will use \"input feeding\", i.e. it will concatenate its previous output to its input. Therefore the input size for the decoder should be `tgtembsz+hidden`.\n",
    "* Only `numLayers`, `dropout`, and `bidirectional` keyword arguments should be used for RNNs, leave everything else default.\n",
    "* The memory parameter `w` is used to convert encoder states to keys. If the encoder is bidirectional initialize it to a `(hidden,2*hidden)` parameter, otherwise set it to the constant 1.\n",
    "* The attention parameter `wquery` is used to transform the query, set it to the constant 1 for this project.\n",
    "* The attention parameter `scale` is used to scale the attention scores before softmax, set it to a parameter of size 1.\n",
    "* The attention parameter `wattn` is used to transform the concatenation of the decoder output and the context vector to the attention vector. It should be a parameter of size `(hidden,2*hidden)` if unidirectional, `(hidden,3*hidden)` if bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function S2S(hidden::Int, srcembsz::Int, tgtembsz::Int, srcvocab::Vocab, tgtvocab::Vocab;\n",
    "             layers=1, bidirectional=false, dropout=0)\n",
    "    \n",
    "    srcembed = Embed(length(srcvocab.i2w), srcembsz)\n",
    "    encoder = RNN(srcembsz, hidden, numLayers=(bidirectional ? Integer(layers/2) : layers), bidirectional=bidirectional, dropout=dropout, dataType=Float32, usegpu=(gpu()>=0))\n",
    "    memory = (bidirectional ? Memory(param(hidden,2*hidden)) : Memory(1))\n",
    "    tgtembed = Embed(length(tgtvocab.i2w), tgtembsz)\n",
    "    decoder = RNN(tgtembsz+hidden, hidden, numLayers=layers, dropout=dropout, dataType=Float32, usegpu=(gpu()>=0))\n",
    "    attention = Attention(1, (bidirectional ? param(hidden,3*hidden) : param(hidden,2*hidden)), param(1))\n",
    "    projection = Linear(hidden, length(tgtvocab.i2w))\n",
    "    \n",
    "    return S2S(srcembed, encoder, memory, tgtembed, decoder, attention,\n",
    "        projection, dropout, srcvocab, tgtvocab)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and data\n",
    "\n",
    "We will load a pretrained model (16.20 bleu) for code testing.  The data should be loaded\n",
    "with the vocabulary from the pretrained model for word id consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading reference model\n",
      "└ @ Main In[15]:2\n",
      "┌ Info: Reading data\n",
      "└ @ Main In[15]:14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MTData(TextReader(\"datasets/tr_to_en/tr.test\", Vocab(Dict(\"dev\" => 1277,\"komuta\" => 13566,\"ellisi\" => 25239,\"adresini\" => 22820,\"yüzeyi\" => 4051,\"paris'te\" => 9494,\"kafamdaki\" => 18790,\"yüzeyinde\" => 5042,\"geçerlidir\" => 6612,\"kökten\" => 7774…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"seçmemiz\", \"destekleyip\", \"karşılaştırılabilir\", \"ördeğin\", \"gününüzü\", \"bağışçı\", \"istismara\", \"yaşça\", \"tedci\", \"fakültesi'nde\"], 2, 1, split)), TextReader(\"datasets/tr_to_en/en.test\", Vocab(Dict(\"middle-income\" => 13398,\"photosynthesis\" => 7689,\"polarizing\" => 17881,\"henry\" => 4248,\"abducted\" => 15691,\"rises\" => 6225,\"hampshire\" => 13888,\"whiz\" => 16835,\"cost-benefit\" => 13137,\"progression\" => 5549…), [\"<s>\", \"<unk>\", \",\", \".\", \"the\", \"and\", \"to\", \"of\", \"a\", \"that\"  …  \"archaea\", \"handshake\", \"brit\", \"wiper\", \"heroines\", \"coca\", \"exceptionally\", \"gallbladder\", \"autopsies\", \"linguistics\"], 2, 1, split)), 64, 9223372036854775807, false, 10, Array{Any,1}[[], [], [], [], [], [], [], [], [], []  …  [], [], [], [], [], [], [], [], [], []], arraybatch)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if !isdefined(Main, :pretrained) || pretrained === nothing\n",
    "    @info \"Loading reference model\"\n",
    "    isfile(\"s2smodel.jld2\") || download(\"http://people.csail.mit.edu/deniz/comp542/s2smodel.jld2\",\"s2smodel.jld2\")\n",
    "    pretrained = Knet.load(\"s2smodel.jld2\",\"model\")\n",
    "end\n",
    "datadir = \"datasets/tr_to_en\"\n",
    "if !isdir(datadir)\n",
    "    @info \"Downloading data\"\n",
    "    download(\"http://www.phontron.com/data/qi18naacl-dataset.tar.gz\", \"qi18naacl-dataset.tar.gz\")\n",
    "    run(`tar xzf qi18naacl-dataset.tar.gz`)\n",
    "end\n",
    "if !isdefined(Main, :tr_vocab)\n",
    "    BATCHSIZE, MAXLENGTH = 64, 50\n",
    "    @info \"Reading data\"\n",
    "    tr_vocab = pretrained.srcvocab # Vocab(\"$datadir/tr.train\", mincount=5)\n",
    "    en_vocab = pretrained.tgtvocab # Vocab(\"$datadir/en.train\", mincount=5)\n",
    "    tr_train = TextReader(\"$datadir/tr.train\", tr_vocab)\n",
    "    en_train = TextReader(\"$datadir/en.train\", en_vocab)\n",
    "    tr_dev = TextReader(\"$datadir/tr.dev\", tr_vocab)\n",
    "    en_dev = TextReader(\"$datadir/en.dev\", en_vocab)\n",
    "    tr_test = TextReader(\"$datadir/tr.test\", tr_vocab)\n",
    "    en_test = TextReader(\"$datadir/en.test\", en_vocab)\n",
    "    dtrn = MTData(tr_train, en_train, batchsize=BATCHSIZE, maxlength=MAXLENGTH)\n",
    "    ddev = MTData(tr_dev, en_dev, batchsize=BATCHSIZE)\n",
    "    dtst = MTData(tr_test, en_test, batchsize=BATCHSIZE)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:           | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing S2S constructor | \u001b[32m  16  \u001b[39m\u001b[36m   16\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing S2S constructor\", Any[], 16, false)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing S2S constructor\" begin\n",
    "    H,Ex,Ey,Vx,Vy,L,Dx,Pdrop = 8,9,10,length(dtrn.src.vocab.i2w),length(dtrn.tgt.vocab.i2w),2,2,0.2\n",
    "    m = S2S(H,Ex,Ey,dtrn.src.vocab,dtrn.tgt.vocab;layers=L,bidirectional=(Dx==2),dropout=Pdrop)\n",
    "    @test size(m.srcembed.w) == (Ex,Vx)\n",
    "    @test size(m.tgtembed.w) == (Ey,Vy)\n",
    "    @test m.encoder.inputSize == Ex\n",
    "    @test m.decoder.inputSize == Ey + H\n",
    "    @test m.encoder.hiddenSize == m.decoder.hiddenSize == H\n",
    "    @test m.encoder.direction == Dx-1\n",
    "    @test m.encoder.numLayers == (Dx == 2 ? L÷2 : L)\n",
    "    @test m.decoder.numLayers == L\n",
    "    @test m.encoder.dropout == m.decoder.dropout == Pdrop\n",
    "    @test size(m.projection.w) == (Vy,H)\n",
    "    @test size(m.memory.w) == (Dx == 2 ? (H,2H) : ())\n",
    "    @test m.attention.wquery == 1\n",
    "    @test size(m.attention.wattn) == (Dx == 2 ? (H,3H) : (H,2H))\n",
    "    @test size(m.attention.scale) == (1,)\n",
    "    @test m.srcvocab === dtrn.src.vocab\n",
    "    @test m.tgtvocab === dtrn.tgt.vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Memory\n",
    "\n",
    "The memory layer turns the output of the encoder to a pair of tensors that will be used as\n",
    "keys and values for the attention mechanism. Remember that the encoder RNN output has size\n",
    "`(H*D,B,Tx)` where `H` is the hidden size, `D` is 1 for unidirectional, 2 for\n",
    "bidirectional, `B` is the batchsize, and `Tx` is the sequence length. It will be\n",
    "convenient to store these values in batch major form for the attention mechanism, so\n",
    "*values* in memory will be a permuted copy of the encoder output with size `(H*D,Tx,B)`\n",
    "(see `@doc permutedims`). The *keys* in the memory need to have the same first dimension\n",
    "as the *queries* (i.e. the decoder hidden states). So *values* will be transformed into\n",
    "*keys* of size `(H,B,Tx)` with `keys = m.w * values` where `m::Memory` is the memory\n",
    "layer. Note that you will have to do some reshaping to 2-D and back to 3-D for matrix\n",
    "multiplications. Also note that `m.w` may be a scalar such as `1` e.g. when `D=1` and we\n",
    "want keys and values to be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (m::Memory)(x)\n",
    "    v = permutedims(x,(1,3,2)) # H*D,Tx,B\n",
    "    k = m.w == 1 ? v : reshape(m.w * reshape(v,size(v,1),:), (:, size(v)[2:end]...))\n",
    "    return k, v\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following helper function for scaling and linear transformations of 3-D tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mmul (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmul(w,x) = (w == 1 ? x : w == 0 ? 0 : reshape(w * reshape(x,size(x,1),:), (:, size(x)[2:end]...)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...\n",
      "└ @ CUDAnative /kuacc/users/mabdullatif18/.julia/packages/CUDAnative/3Jwj2/src/compiler/rtlib.jl:188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:  | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing memory | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing memory\", Any[], 2, false)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing memory\" begin\n",
    "    H,D,B,Tx = pretrained.encoder.hiddenSize, pretrained.encoder.direction+1, 4, 5\n",
    "    x = KnetArray(randn(Float32,H*D,B,Tx))\n",
    "    k,v = pretrained.memory(x)\n",
    "    @test v == permutedims(x,(1,3,2))\n",
    "    @test k == mmul(pretrained.memory.w, v)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Encoder\n",
    "\n",
    "`encode()` takes a model `s` and a source language minibatch `src`. It passes the input\n",
    "through `s.srcembed` and `s.encoder` layers with the `s.encoder` RNN hidden states\n",
    "initialized to `0` in the beginning, and copied to the `s.decoder` RNN at the end. The\n",
    "steps so far are identical to `S2S_v1` but there is an extra step: The encoder output is\n",
    "passed to the `s.memory` layer which returns a `(keys,values)` pair. `encode()` returns\n",
    "this pair to be used later by the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encode (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function encode(s::S2S, src)\n",
    "    source_embedding = s.srcembed(src)\n",
    "    s.encoder.h = 0\n",
    "    s.encoder.c = 0\n",
    "    enc_out = s.encoder(source_embedding)\n",
    "    s.decoder.h = s.encoder.h\n",
    "    s.decoder.c = s.encoder.c\n",
    "    k, v = s.memory(enc_out)\n",
    "    return k, v\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:   | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing encoder | \u001b[32m   7  \u001b[39m\u001b[36m    7\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing encoder\", Any[], 7, false)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing encoder\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    key1,val1 = encode(pretrained, src1)\n",
    "    H,D,B,Tx = pretrained.encoder.hiddenSize, pretrained.encoder.direction+1, size(src1,1), size(src1,2)\n",
    "    @test size(key1) == (H,Tx,B)\n",
    "    @test size(val1) == (H*D,Tx,B)\n",
    "    @test (pretrained.decoder.h,pretrained.decoder.c) === (pretrained.encoder.h,pretrained.encoder.c)\n",
    "    @test norm(key1) ≈ 1214.4755f0\n",
    "    @test norm(val1) ≈ 191.10411f0\n",
    "    @test norm(pretrained.decoder.h) ≈ 48.536964f0\n",
    "    @test norm(pretrained.decoder.c) ≈ 391.69028f0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attention parameter wquery is used to transform the query, set it to the constant 1 for this project.\n",
    "\n",
    "# The attention parameter wattn is used to transform the concatenation of the decoder output \n",
    "#     and the context vector to the attention vector. It should be \n",
    "#     a parameter of size (hidden,2*hidden) if unidirectional, (hidden,3*hidden) if bidirectional.\n",
    "\n",
    "# The attention parameter scale is used to scale the attention scores before softmax, \n",
    "#     set it to a parameter of size 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Attention\n",
    "\n",
    "The attention layer takes `cell`: the decoder output, and `mem`: a pair of (keys,vals)\n",
    "from the encoder, and computes and returns the attention vector. First `a.wquery` is used\n",
    "to linearly transform the cell to the query tensor. The query tensor is reshaped and/or\n",
    "permuted as appropriate and multiplied with the keys tensor to compute the attention\n",
    "scores. Please see `@doc bmm` for the batched matrix multiply operation used for this\n",
    "step. The attention scores are scaled using `a.scale` and normalized along the time\n",
    "dimension using `softmax`. After the appropriate reshape and/or permutation, the scores\n",
    "are multiplied with the `vals` tensor (using `bmm` again) to compute the context\n",
    "tensor. After the appropriate reshape and/or permutation the context vector is\n",
    "concatenated with the cell and linearly transformed to the attention vector using\n",
    "`a.wattn`. Please see the paper and code examples for details.\n",
    "\n",
    "Note: the paper mentions a final `tanh` transform, however the final version of the\n",
    "reference code does not use `tanh` and gets better results. Therefore we will skip `tanh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (a::Attention)(cell, mem) # deccell(H,B,Ty), ( keys(H,Tx,B), vals(Dx*H,Tx,B) ) -> attnvec(H,B,Ty)\n",
    "    if a.wquery == 1\n",
    "        query = cell\n",
    "    else\n",
    "        cell_reshaped = reshape(cell, (size(cell,1), size(cell,2)*size(cell,3))) # H, B*Ty\n",
    "        query = a.wquery * cell_reshaped\n",
    "        query = reshape(query, (size(query,1), size(cell,2), size(cell,3)))\n",
    "    end\n",
    "    k, v = mem\n",
    "    attn_scores = a.scale .* bmm(permutedims(query, (3,1,2)), k) # Ty,H,B # H,Tx,B --> Ty, Tx, B\n",
    "    attn_scores = softmax(attn_scores, dims = 2) \n",
    "    cntxt = bmm(v, permutedims(attn_scores, (2,1,3))) # Dx*H,Tx,B # Tx, Ty, B  --> Dx*H, Ty, B\n",
    "    attn_vec = a.wattn * reshape(vcat(cell, permutedims(cntxt, (1,3,2))), (:, size(cell,2)*size(cell,3))) # H, 3H  # 3H, B*Ty\n",
    "    attn_vec = reshape(attn_vec, (size(cell,1), size(cell,2), size(cell,3)))\n",
    "    return attn_vec # H,B,Ty\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:     | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing attention | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing attention\", Any[], 2, false)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing attention\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    key1,val1 = encode(pretrained, src1)\n",
    "    H,B = pretrained.encoder.hiddenSize, size(src1,1)\n",
    "    Knet.seed!(1)\n",
    "    x = KnetArray(randn(Float32,H,B,5))\n",
    "    y = pretrained.attention(x, (key1, val1))\n",
    "    @test size(y) == size(x)\n",
    "    @test norm(y) ≈ 808.381f0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Decoder\n",
    "\n",
    "`decode()` takes a model `s`, a target language minibatch `tgt`, the memory from the\n",
    "encoder `mem` and the decoder output from the previous time step `prev`. After the input\n",
    "is passed through the embedding layer, it is concatenated with `prev` (this is called\n",
    "input feeding). The resulting tensor is passed through `s.decoder`. Finally the\n",
    "`s.attention` layer takes the decoder output and the encoder memory to compute the\n",
    "\"attention vector\" which is returned by `decode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decode (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function decode(s::S2S, tgt, mem, prev) # tgtembed(Ey,B,Ty) . attnvec(H,B,Ty)[t-1] = (Ey+H,B,Ty) -> deccell(H,B,Ty)\n",
    "#     @show prev\n",
    "    tgt_embedding = s.tgtembed(tgt)\n",
    "    dec_in = vcat(tgt_embedding, prev)\n",
    "    deccell = s.decoder(dec_in)\n",
    "    attn_vec = s.attention(deccell, mem)\n",
    "#     @show attn_vec\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:   | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing decoder | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing decoder\", Any[], 2, false)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing decoder\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    key1,val1 = encode(pretrained, src1)\n",
    "    H,B = pretrained.encoder.hiddenSize, size(src1,1)\n",
    "    Knet.seed!(1)\n",
    "    cell = randn!(similar(key1, size(key1,1), size(key1,3), 1))\n",
    "    cell = decode(pretrained, tgt1[:,1:1], (key1,val1), cell)\n",
    "    @test size(cell) == (H,B,1)\n",
    "    @test norm(cell) ≈ 131.21631f0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Loss\n",
    "\n",
    "The loss function takes source language minibatch `src`, and a target language minibatch\n",
    "`tgt` and returns `sumloss/numwords` if `average=true` or `(sumloss,numwords)` if\n",
    "`average=false` where `sumloss` is the total negative log likelihood loss and `numwords` is\n",
    "the number of words predicted (including a final eos for each sentence). The source is first\n",
    "encoded using `encode` yielding a `(keys,vals)` pair (memory). Then the decoder is called to\n",
    "predict each word of `tgt` given the previous word, `(keys,vals)` pair, and the previous\n",
    "decoder output. The previous decoder output is initialized with zeros for the first\n",
    "step. The output of the decoder at each step is passed through the projection layer giving\n",
    "word scores. Losses can be computed from word scores and masked/shifted `tgt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S)(src, tgt; average=true) # src(B, Tx), tgt(B, Ty)\n",
    "    mem = encode(s, src)\n",
    "    prev = KnetArray{Float32}(zeros(size(mem[1],1), size(src,1), 1))\n",
    "    all_scores = []\n",
    "    for i in 1:size(tgt, 2)-1\n",
    "        prev = decode(s, tgt[:,i:i], mem, prev) # H, B, 1\n",
    "        scores = s.projection(reshape(prev, size(prev)[1:2])) # H, B --> V, B\n",
    "        push!(all_scores, scores)\n",
    "    end\n",
    "    all_scores = hcat(all_scores...) # V, Ty*B\n",
    "    answers = tgt[:, 2:end]\n",
    "    mask!(answers, s.tgtvocab.eos)\n",
    "    answers = reshape(answers, :)\n",
    "    return nll(all_scores, answers, average=average)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mTesting loss: \u001b[39m\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[29]:4\u001b[22m\n",
      "  Expression: pretrained(src1, tgt1, average=false) == (1949.1901f0, 1329)\n",
      "   Evaluated: (1949.1897f0, 1329) == (1949.1901f0, 1329)\n",
      "Stacktrace:\n",
      " [1] top-level scope at \u001b[1mIn[29]:4\u001b[22m\n",
      " [2] top-level scope at \u001b[1m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:1113\u001b[22m\n",
      " [3] top-level scope at \u001b[1mIn[29]:2\u001b[22m\n",
      "\u001b[37m\u001b[1mTest Summary: | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[91m\u001b[1mFail  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing loss  | \u001b[32m   1  \u001b[39m\u001b[91m   1  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "ename": "TestSetException",
     "evalue": "Some tests did not pass: 1 passed, 1 failed, 0 errored, 0 broken.",
     "output_type": "error",
     "traceback": [
      "Some tests did not pass: 1 passed, 1 failed, 0 errored, 0 broken.",
      "",
      "Stacktrace:",
      " [1] finish(::Test.DefaultTestSet) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:877",
      " [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:1123",
      " [3] top-level scope at In[29]:2"
     ]
    }
   ],
   "source": [
    "@testset \"Testing loss\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    @test pretrained(src1,tgt1) ≈ 1.4666592f0\n",
    "    @test pretrained(src1,tgt1,average=false) == (1949.1901f0, 1329)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7. Greedy translator\n",
    "\n",
    "An `S2S` object can be called with a single argument (source language minibatch `src`, with\n",
    "size `B,Tx`) to generate translations (target language minibatch with size `B,Ty`). The\n",
    "keyword argument `stopfactor` determines how much longer the output can be compared to the\n",
    "input. Similar to the loss function, the source minibatch is encoded yield a `(keys,vals)`\n",
    "pair (memory). We generate the output one time step at a time by calling the decoder with\n",
    "the last output, the memory, and the last decoder state. The last output is initialized to\n",
    "an array of `eos` tokens and the last decoder state is initialized to an array of\n",
    "zeros. After computing the scores for the next word using the projection layer, the highest\n",
    "scoring words are selected and appended to the output. The generation stops when all outputs\n",
    "in the batch have generated `eos` or when the length of the output is `stopfactor` times the\n",
    "input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S)(src; stopfactor = 3) # src(B, Tx)\n",
    "    B = size(src, 1) # batch size\n",
    "    eos_ind = s.tgtvocab.eos # eos token (we need it for the stopping condition)\n",
    "    src_len = size(src, 2) # source maximum sentence length\n",
    "    mem = encode(s, src)\n",
    "    prev = KnetArray{Float32}(zeros(size(mem[1],1), size(src,1), 1))\n",
    "    tgtvec = repeat([eos_ind], B, 1)\n",
    "    tgt = []\n",
    "    # stopping condition:\n",
    "    # 1- size(tgt, 2) >= stopfactor * src_len\n",
    "    # 2- if all batches have generate eos \n",
    "    eos_generated = zeros(B) # zero if hasn't generated eos, one if eos has been generated\n",
    "    while true # for each decoder timestep\n",
    "        prev = decode(s, tgtvec, mem, prev) # H, B, 1\n",
    "        # feed it to the projection layer and get scores over vocab # dims: V, B\n",
    "        scores = s.projection(reshape(prev, size(prev)[1:2])) # H, B --> V, B\n",
    "        # get the index of the maximum score for each batch element # dims: B\n",
    "        tgtvec = reshape(map(x -> x[1], argmax(scores, dims=1)), (:,1))\n",
    "        eoses = findall(x->x==eos_ind, tgtvec)\n",
    "        eos_generated[eoses] .= 1\n",
    "        push!(tgt, tgtvec)\n",
    "        if (length(tgt) >= stopfactor * src_len) || all(eos_generated .== 1)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    tgt = hcat(tgt...)\n",
    "    return tgt # dims: B, Ty\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:      | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing translator | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing translator\", Any[], 2, false)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing translator\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    tgt2 = pretrained(src1)\n",
    "    @test size(tgt2) == (64, 41)\n",
    "    @test tgt2[1:3,1:3] == [14 25 10647; 37 25 1426; 27 5 349]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8. Training\n",
    "\n",
    "`trainmodel` creates, trains and returns an `S2S` model. The arguments are described in\n",
    "comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainmodel (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trainmodel(trn,                  # Training data\n",
    "                    dev,                  # Validation data, used to determine the best model\n",
    "                    tst...;               # Zero or more test datasets, their loss will be periodically reported\n",
    "                    bidirectional = true, # Whether to use a bidirectional encoder\n",
    "                    layers = 2,           # Number of layers (use `layers÷2` for a bidirectional encoder)\n",
    "                    hidden = 512,         # Size of the hidden vectors\n",
    "                    srcembed = 512,       # Size of the source language embedding vectors\n",
    "                    tgtembed = 512,       # Size of the target language embedding vectors\n",
    "                    dropout = 0.2,        # Dropout probability\n",
    "                    epochs = 0,           # Number of epochs (one of epochs or iters should be nonzero for training)\n",
    "                    iters = 0,            # Number of iterations (one of epochs or iters should be nonzero for training)\n",
    "                    bleu = false,         # Whether to calculate the BLEU score for the final model\n",
    "                    save = false,         # Whether to save the final model\n",
    "                    seconds = 60,         # Frequency of progress reporting\n",
    "                    )\n",
    "    @show bidirectional, layers, hidden, srcembed, tgtembed, dropout, epochs, iters, bleu, save; flush(stdout)\n",
    "    model = S2S(hidden, srcembed, tgtembed, trn.src.vocab, trn.tgt.vocab;\n",
    "                layers=layers, dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "    epochs == iters == 0 && return model\n",
    "\n",
    "    (ctrn,cdev,ctst) = collect(trn),collect(dev),collect.(tst)\n",
    "    traindata = (epochs > 0\n",
    "                 ? collect(flatten(shuffle!(ctrn) for i in 1:epochs))\n",
    "                 : shuffle!(collect(take(cycle(ctrn), iters))))\n",
    "\n",
    "    bestloss, bestmodel = loss(model, cdev), deepcopy(model)\n",
    "    progress!(adam(model, traindata), seconds=seconds) do y\n",
    "        devloss = loss(model, cdev)\n",
    "        tstloss = map(d->loss(model,d), ctst)\n",
    "        if devloss < bestloss\n",
    "            bestloss, bestmodel = devloss, deepcopy(model)\n",
    "        end\n",
    "        println(stderr)\n",
    "        (dev=devloss, tst=tstloss, mem=Float32(CuArrays.usage[]))\n",
    "    end\n",
    "    save && Knet.save(\"attn-$(Int(time_ns())).jld2\", \"model\", bestmodel)\n",
    "    bleu && Main.bleu(bestmodel,dev)\n",
    "    return bestmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model: If your implementation is correct, the first epoch should take about 24\n",
    "minutes on a v100 and bring the loss from 9.83 to under 4.0. 10 epochs would take about 4\n",
    "hours on a v100. With other GPUs you may have to use a smaller batch size (if memory is\n",
    "lower) and longer time (if gpu speed is lower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bidirectional, layers, hidden, srcembed, tgtembed, dropout, epochs, iters, bleu, save) = (true, 2, 512, 512, 512, 0.2, 10, 0, true, true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "┣                    ┫ [0.00%, 1/28120, 00:15/118:50:04, 15.21s/i] (dev = 9.83582f0, tst = (9.836524f0,), mem = 3.2419828f10)\n",
      "┣▏                   ┫ [0.97%, 274/28120, 01:19/02:14:47, 4.29i/s] (dev = 5.412611f0, tst = (5.4172993f0,), mem = 3.302986f10)\n",
      "┣▍                   ┫ [2.09%, 589/28120, 02:22/01:53:23, 4.95i/s] (dev = 4.998573f0, tst = (5.0214133f0,), mem = 3.1957395f10)\n",
      "┣▋                   ┫ [3.21%, 902/28120, 03:26/01:47:09, 4.91i/s] (dev = 4.8109946f0, tst = (4.8308487f0,), mem = 3.308382f10)\n",
      "┣▊                   ┫ [4.30%, 1210/28120, 04:30/01:44:32, 4.84i/s] (dev = 4.6728315f0, tst = (4.677597f0,), mem = 3.2713085f10)\n",
      "┣█                   ┫ [5.39%, 1515/28120, 05:34/01:43:14, 4.78i/s] (dev = 4.5607166f0, tst = (4.5677443f0,), mem = 3.2973533f10)\n",
      "┣█▎                  ┫ [6.55%, 1843/28120, 06:37/01:41:03, 5.15i/s] (dev = 4.439365f0, tst = (4.4767203f0,), mem = 3.1693791f10)\n",
      "┣█▌                  ┫ [7.62%, 2144/28120, 07:41/01:40:49, 4.72i/s] (dev = 4.3662596f0, tst = (4.314896f0,), mem = 3.1236608f10)\n",
      "┣█▊                  ┫ [8.76%, 2462/28120, 08:44/01:39:50, 5.02i/s] (dev = 4.26401f0, tst = (4.2557864f0,), mem = 3.2420753f10)\n",
      "┣█▉                  ┫ [9.90%, 2785/28120, 09:48/01:38:57, 5.09i/s] (dev = 4.2038884f0, tst = (4.1848054f0,), mem = 3.2947245f10)\n",
      "┣██▏                 ┫ [11.02%, 3098/28120, 10:52/01:38:36, 4.91i/s] (dev = 4.097676f0, tst = (4.073543f0,), mem = 3.3055863f10)\n",
      "┣██▍                 ┫ [12.13%, 3412/28120, 11:55/01:38:14, 4.95i/s] (dev = 3.9488688f0, tst = (3.9522212f0,), mem = 3.2683817f10)\n",
      "┣██▋                 ┫ [13.22%, 3718/28120, 12:59/01:38:13, 4.78i/s] (dev = 3.845863f0, tst = (3.8533783f0,), mem = 3.171563f10)\n",
      "┣██▊                 ┫ [14.32%, 4028/28120, 14:03/01:38:03, 4.89i/s] (dev = 3.6762831f0, tst = (3.7178612f0,), mem = 3.285884f10)\n",
      "┣███                 ┫ [15.47%, 4350/28120, 15:06/01:37:39, 5.05i/s] (dev = 3.5647116f0, tst = (3.553808f0,), mem = 3.3012326f10)\n",
      "┣███▎                ┫ [16.55%, 4654/28120, 16:10/01:37:41, 4.78i/s] (dev = 3.4511964f0, tst = (3.3314857f0,), mem = 3.3024698f10)\n",
      "┣███▌                ┫ [17.70%, 4977/28120, 17:13/01:37:18, 5.10i/s] (dev = 3.3837492f0, tst = (3.172302f0,), mem = 3.2433064f10)\n",
      "┣███▊                ┫ [18.81%, 5288/28120, 18:17/01:37:14, 4.87i/s] (dev = 3.268673f0, tst = (3.0554676f0,), mem = 3.2390787f10)\n",
      "┣███▉                ┫ [19.98%, 5619/28120, 19:21/01:36:50, 5.19i/s] (dev = 3.1983955f0, tst = (3.0398588f0,), mem = 3.3067549f10)\n",
      "┣████▏               ┫ [21.09%, 5931/28120, 20:24/01:36:45, 4.92i/s] (dev = 3.1685326f0, tst = (2.947494f0,), mem = 3.2669264f10)\n",
      "┣████▍               ┫ [22.25%, 6258/28120, 21:28/01:36:26, 5.17i/s] (dev = 3.1236238f0, tst = (2.9572597f0,), mem = 3.2681662f10)\n",
      "┣████▋               ┫ [23.36%, 6568/28120, 22:31/01:36:26, 4.85i/s] (dev = 3.083699f0, tst = (2.7951095f0,), mem = 3.2378085f10)\n",
      "┣████▉               ┫ [24.50%, 6888/28120, 23:36/01:36:19, 4.99i/s] (dev = 3.049705f0, tst = (2.7468295f0,), mem = 3.1907256f10)\n",
      "┣█████               ┫ [25.60%, 7200/28120, 24:39/01:36:16, 4.93i/s] (dev = 3.0022337f0, tst = (2.698111f0,), mem = 3.2425212f10)\n",
      "┣█████▎              ┫ [26.73%, 7516/28120, 25:42/01:36:10, 4.99i/s] (dev = 2.983014f0, tst = (2.6244707f0,), mem = 3.3009125f10)\n",
      "┣█████▌              ┫ [27.85%, 7831/28120, 26:46/01:36:06, 4.97i/s] (dev = 2.9427862f0, tst = (2.608652f0,), mem = 3.2683897f10)\n",
      "┣█████▊              ┫ [29.07%, 8175/28120, 27:49/01:35:41, 5.43i/s] (dev = 2.948742f0, tst = (2.6095865f0,), mem = 3.2026698f10)\n",
      "┣██████              ┫ [30.23%, 8501/28120, 28:52/01:35:31, 5.14i/s] (dev = 2.9165637f0, tst = (2.5361578f0,), mem = 3.283717f10)\n",
      "┣██████▎             ┫ [31.38%, 8824/28120, 29:56/01:35:23, 5.08i/s] (dev = 2.9068267f0, tst = (2.4933202f0,), mem = 3.3069564f10)\n",
      "┣██████▌             ┫ [32.54%, 9150/28120, 30:59/01:35:14, 5.16i/s] (dev = 2.9100764f0, tst = (2.400312f0,), mem = 3.227752f10)\n",
      "┣██████▋             ┫ [33.73%, 9485/28120, 32:03/01:35:00, 5.28i/s] (dev = 2.9126153f0, tst = (2.4336386f0,), mem = 3.22702f10)\n",
      "┣██████▉             ┫ [34.90%, 9813/28120, 33:06/01:34:51, 5.18i/s] (dev = 2.8834116f0, tst = (2.4420123f0,), mem = 3.267936f10)\n",
      "┣███████▏            ┫ [35.99%, 10120/28120, 34:09/01:34:55, 4.84i/s] (dev = 2.865707f0, tst = (2.4351563f0,), mem = 3.267313f10)\n",
      "┣███████▍            ┫ [37.11%, 10436/28120, 35:13/01:34:53, 5.00i/s] (dev = 2.8507485f0, tst = (2.4378538f0,), mem = 3.1981631f10)\n",
      "┣███████▋            ┫ [38.26%, 10758/28120, 36:16/01:34:48, 5.08i/s] (dev = 2.8309479f0, tst = (2.4147942f0,), mem = 3.2642454f10)\n",
      "┣███████▊            ┫ [39.37%, 11072/28120, 37:19/01:34:47, 4.97i/s] (dev = 2.819458f0, tst = (2.384046f0,), mem = 3.2653865f10)\n",
      "┣████████▏           ┫ [40.63%, 11426/28120, 38:24/01:34:30, 5.48i/s] (dev = 2.8665755f0, tst = (2.2641766f0,), mem = 3.0763897f10)\n",
      "┣████████▎           ┫ [41.82%, 11761/28120, 39:27/01:34:20, 5.29i/s] (dev = 2.8643498f0, tst = (2.3012357f0,), mem = 3.2476514f10)\n",
      "┣████████▌           ┫ [42.97%, 12082/28120, 40:30/01:34:17, 5.08i/s] (dev = 2.8707168f0, tst = (2.304493f0,), mem = 3.2263164f10)\n",
      "┣████████▊           ┫ [44.11%, 12405/28120, 41:34/01:34:13, 5.11i/s] (dev = 2.859122f0, tst = (2.311966f0,), mem = 3.2267016f10)\n",
      "┣█████████           ┫ [45.27%, 12731/28120, 42:37/01:34:08, 5.13i/s] (dev = 2.8363008f0, tst = (2.272328f0,), mem = 3.1941433f10)\n",
      "┣█████████▎          ┫ [46.40%, 13048/28120, 43:41/01:34:09, 4.96i/s] (dev = 2.826255f0, tst = (2.2547598f0,), mem = 3.1343434f10)\n",
      "┣█████████▌          ┫ [47.54%, 13369/28120, 44:45/01:34:07, 5.05i/s] (dev = 2.8108609f0, tst = (2.1631072f0,), mem = 3.238579f10)\n",
      "┣█████████▋          ┫ [48.66%, 13682/28120, 45:49/01:34:10, 4.84i/s] (dev = 2.8045886f0, tst = (2.1757421f0,), mem = 3.1852878f10)\n",
      "┣█████████▉          ┫ [49.80%, 14005/28120, 46:52/01:34:07, 5.11i/s] (dev = 2.7951832f0, tst = (2.0555227f0,), mem = 3.2396917f10)\n",
      "┣██████████▏         ┫ [50.98%, 14336/28120, 47:56/01:34:01, 5.21i/s] (dev = 2.8613973f0, tst = (2.0707622f0,), mem = 3.2592126f10)\n",
      "┣██████████▍         ┫ [52.13%, 14659/28120, 49:01/01:34:01, 5.00i/s] (dev = 2.8652508f0, tst = (2.0283465f0,), mem = 3.0582565f10)\n",
      "┣██████████▋         ┫ [53.27%, 14980/28120, 50:04/01:33:59, 5.06i/s] (dev = 2.8603287f0, tst = (2.0632343f0,), mem = 3.2593658f10)\n",
      "┣██████████▉         ┫ [54.47%, 15318/28120, 51:07/01:33:51, 5.33i/s] (dev = 2.8459542f0, tst = (2.0064182f0,), mem = 3.214038f10)\n",
      "┣███████████         ┫ [55.58%, 15630/28120, 52:11/01:33:53, 4.91i/s] (dev = 2.8424757f0, tst = (2.0103276f0,), mem = 3.2027132f10)\n",
      "┣███████████▎        ┫ [56.64%, 15928/28120, 53:16/01:34:02, 4.60i/s] (dev = 2.8245285f0, tst = (1.9474512f0,), mem = 3.075139f10)\n",
      "┣███████████▌        ┫ [57.81%, 16257/28120, 54:19/01:33:57, 5.20i/s] (dev = 2.8561294f0, tst = (1.9945297f0,), mem = 3.254917f10)\n",
      "┣███████████▊        ┫ [59.02%, 16596/28120, 55:22/01:33:49, 5.36i/s] (dev = 2.8413885f0, tst = (1.9652447f0,), mem = 3.2257962f10)\n",
      "┣████████████        ┫ [60.12%, 16906/28120, 56:26/01:33:51, 4.90i/s] (dev = 2.866561f0, tst = (1.9033576f0,), mem = 3.2290732f10)\n",
      "┣████████████▎       ┫ [61.29%, 17234/28120, 57:29/01:33:47, 5.18i/s] (dev = 2.8924134f0, tst = (1.8257759f0,), mem = 3.2334033f10)\n",
      "┣████████████▍       ┫ [62.43%, 17554/28120, 58:32/01:33:46, 5.05i/s] (dev = 2.8833647f0, tst = (1.8305432f0,), mem = 3.1842066f10)\n",
      "┣████████████▋       ┫ [63.60%, 17884/28120, 59:36/01:33:42, 5.21i/s] (dev = 2.9055028f0, tst = (1.868711f0,), mem = 3.123112f10)\n",
      "┣████████████▉       ┫ [64.78%, 18215/28120, 01:00:39/01:33:38, 5.22i/s] (dev = 2.8768141f0, tst = (1.821229f0,), mem = 3.2605987f10)\n",
      "┣█████████████▏      ┫ [65.91%, 18533/28120, 01:01:42/01:33:37, 5.03i/s] (dev = 2.8835948f0, tst = (1.7998776f0,), mem = 3.2122065f10)\n",
      "┣█████████████▍      ┫ [67.00%, 18839/28120, 01:02:46/01:33:41, 4.81i/s] (dev = 2.8748791f0, tst = (1.8033469f0,), mem = 3.2105286f10)\n",
      "┣█████████████▋      ┫ [68.15%, 19165/28120, 01:03:49/01:33:38, 5.14i/s] (dev = 2.8715467f0, tst = (1.7863139f0,), mem = 3.2249967f10)\n",
      "┣█████████████▊      ┫ [69.33%, 19497/28120, 01:04:53/01:33:35, 5.19i/s] (dev = 2.8774736f0, tst = (1.7912016f0,), mem = 3.1266142f10)\n",
      "┣██████████████      ┫ [70.49%, 19822/28120, 01:05:57/01:33:33, 5.10i/s] (dev = 2.9100575f0, tst = (1.7886747f0,), mem = 3.1546147f10)\n",
      "┣██████████████▎     ┫ [71.66%, 20150/28120, 01:07:00/01:33:31, 5.16i/s] (dev = 2.9521534f0, tst = (1.7464298f0,), mem = 3.251107f10)\n",
      "┣██████████████▌     ┫ [72.85%, 20485/28120, 01:08:04/01:33:26, 5.31i/s] (dev = 2.9516242f0, tst = (1.7292982f0,), mem = 3.1240086f10)\n",
      "┣██████████████▊     ┫ [74.04%, 20821/28120, 01:09:07/01:33:21, 5.28i/s] (dev = 2.9439225f0, tst = (1.699201f0,), mem = 3.3022075f10)\n",
      "┣███████████████     ┫ [75.17%, 21139/28120, 01:10:10/01:33:21, 5.02i/s] (dev = 2.9438627f0, tst = (1.682277f0,), mem = 3.2291635f10)\n",
      "┣███████████████▎    ┫ [76.27%, 21447/28120, 01:11:14/01:33:24, 4.83i/s] (dev = 2.9104567f0, tst = (1.6916089f0,), mem = 3.1677325f10)\n",
      "┣███████████████▍    ┫ [77.46%, 21783/28120, 01:12:18/01:33:19, 5.30i/s] (dev = 2.9193282f0, tst = (1.6274564f0,), mem = 3.2602415f10)\n",
      "┣███████████████▋    ┫ [78.61%, 22105/28120, 01:13:21/01:33:18, 5.09i/s] (dev = 2.9132838f0, tst = (1.6463556f0,), mem = 3.1990751f10)\n",
      "┣███████████████▉    ┫ [79.69%, 22410/28120, 01:14:25/01:33:22, 4.79i/s] (dev = 2.9043698f0, tst = (1.6359534f0,), mem = 3.2581446f10)\n",
      "┣████████████████▏   ┫ [80.83%, 22730/28120, 01:15:28/01:33:22, 5.01i/s] (dev = 3.0120387f0, tst = (1.6646492f0,), mem = 3.1462488f10)\n",
      "┣████████████████▍   ┫ [81.96%, 23048/28120, 01:16:32/01:33:23, 5.00i/s] (dev = 3.0162148f0, tst = (1.6407247f0,), mem = 3.225209f10)\n",
      "┣████████████████▌   ┫ [83.12%, 23373/28120, 01:17:36/01:33:21, 5.12i/s] (dev = 3.0245512f0, tst = (1.6829426f0,), mem = 3.2142518f10)\n",
      "┣████████████████▊   ┫ [84.22%, 23683/28120, 01:18:39/01:33:23, 4.88i/s] (dev = 3.0000553f0, tst = (1.6707648f0,), mem = 3.2544483f10)\n",
      "┣█████████████████   ┫ [85.45%, 24028/28120, 01:19:42/01:33:17, 5.44i/s] (dev = 3.0198905f0, tst = (1.6319915f0,), mem = 3.2457589f10)\n",
      "┣█████████████████▎  ┫ [86.62%, 24358/28120, 01:20:46/01:33:14, 5.21i/s] (dev = 2.9893217f0, tst = (1.6545496f0,), mem = 3.233392f10)\n",
      "┣█████████████████▌  ┫ [87.71%, 24665/28120, 01:21:49/01:33:17, 4.83i/s] (dev = 2.9673498f0, tst = (1.6391655f0,), mem = 3.1920701f10)\n",
      "┣█████████████████▊  ┫ [88.83%, 24980/28120, 01:22:53/01:33:18, 4.97i/s] (dev = 2.9719727f0, tst = (1.5595356f0,), mem = 3.1465939f10)\n",
      "┣█████████████████▉  ┫ [90.00%, 25307/28120, 01:23:57/01:33:16, 5.12i/s] (dev = 2.9599752f0, tst = (1.4887911f0,), mem = 3.1870999f10)\n",
      "┣██████████████████▏ ┫ [91.11%, 25620/28120, 01:25:00/01:33:17, 4.96i/s] (dev = 3.0673041f0, tst = (1.3885394f0,), mem = 3.2079233f10)\n",
      "┣██████████████████▍ ┫ [92.27%, 25945/28120, 01:26:03/01:33:16, 5.14i/s] (dev = 3.0517159f0, tst = (1.3757838f0,), mem = 3.2238203f10)\n",
      "┣██████████████████▋ ┫ [93.40%, 26263/28120, 01:27:06/01:33:16, 5.02i/s] (dev = 3.077646f0, tst = (1.3777508f0,), mem = 3.222613f10)\n",
      "┣██████████████████▉ ┫ [94.49%, 26570/28120, 01:28:10/01:33:18, 4.84i/s] (dev = 3.064127f0, tst = (1.4029311f0,), mem = 3.256131f10)\n",
      "┣███████████████████ ┫ [95.60%, 26884/28120, 01:29:13/01:33:19, 4.97i/s] (dev = 3.065247f0, tst = (1.4138365f0,), mem = 3.2224221f10)\n",
      "┣███████████████████▎┫ [96.80%, 27220/28120, 01:30:17/01:33:16, 5.27i/s] (dev = 3.0382526f0, tst = (1.4607683f0,), mem = 3.2764998f10)\n",
      "┣███████████████████▌┫ [97.96%, 27546/28120, 01:31:20/01:33:14, 5.13i/s] (dev = 3.0532424f0, tst = (1.4891056f0,), mem = 3.1202136f10)\n",
      "┣███████████████████▊┫ [99.16%, 27883/28120, 01:32:24/01:33:11, 5.30i/s] (dev = 3.0313215f0, tst = (1.428724f0,), mem = 3.2248898f10)\n",
      "┣████████████████████┫ [100.00%, 28120/28120, 01:33:11/01:33:11, 5.03i/s] (dev = 3.0182173f0, tst = (1.4231287f0,), mem = 3.2583565f10)\n",
      "┣████████████████████┫ [100.00%, 4045/4045, 01:03/01:03, 64.46i/s] \n",
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to the standard locale (\"C\").\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 16.31, 48.3/21.5/11.2/6.1 (BP=1.000, ratio=1.013, hyp_len=83535, ref_len=82502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(KnetArray{Float32,2}(512,38126))), LSTM(input=512,hidden=512,bidirectional,dropout=0.2), Memory(P(KnetArray{Float32,2}(512,1024))), Embed(P(KnetArray{Float32,2}(512,18857))), LSTM(input=1024,hidden=512,layers=2,dropout=0.2), Attention(1, P(KnetArray{Float32,2}(512,1536)), P(KnetArray{Float32,1}(1))), Linear(P(KnetArray{Float32,2}(18857,512)), P(KnetArray{Float32,1}(18857))), 0.2, Vocab(Dict(\"ağacından\" => 35370,\"komuta\" => 13566,\"ellisi\" => 25239,\"adresini\" => 22820,\"yüzeyi\" => 4051,\"paris'te\" => 9494,\"kafamdaki\" => 18790,\"yüzeyinde\" => 5042,\"geçerlidir\" => 6612,\"kökten\" => 7774…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"seçmemiz\", \"destekleyip\", \"karşılaştırılabilir\", \"ördeğin\", \"gününüzü\", \"bağışçı\", \"istismara\", \"yaşça\", \"tedci\", \"fakültesi'nde\"], 2, 1, split), Vocab(Dict(\"middle-income\" => 13398,\"photosynthesis\" => 7689,\"polarizing\" => 17881,\"henry\" => 4248,\"abducted\" => 15691,\"rises\" => 6225,\"hampshire\" => 13888,\"whiz\" => 16835,\"cost-benefit\" => 13137,\"progression\" => 5549…), [\"<s>\", \"<unk>\", \",\", \".\", \"the\", \"and\", \"to\", \"of\", \"a\", \"that\"  …  \"archaea\", \"handshake\", \"brit\", \"wiper\", \"heroines\", \"coca\", \"exceptionally\", \"gallbladder\", \"autopsies\", \"linguistics\"], 2, 1, split))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment the appropriate option for training:\n",
    "# model = pretrained  # Use reference model\n",
    "# model = Knet.load(\"attn-1538395466294882.jld2\", \"model\")  # Load pretrained model\n",
    "model1 = trainmodel(dtrn,ddev,take(dtrn,20); epochs=10, save=true, bleu=true)  # Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to sample translations from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translate_sample (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = MTData(tr_dev, en_dev, batchsize=1) |> collect;\n",
    "function translate_sample(model, data)\n",
    "    (src,tgt) = rand(data)\n",
    "    out = model(src)\n",
    "    println(\"SRC: \", int2str(src,model.srcvocab))\n",
    "    println(\"REF: \", int2str(tgt,model.tgtvocab))\n",
    "    println(\"OUT: \", int2str(out,model.tgtvocab))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate translations for random instances from the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: ve sonra , bunun gibi olur — ( <unk> ! ) <unk> ... ( gülüşmeler ) eminim bu şeyin nasıl çalıştığını çözmeye çalışıyorsunuz .\n",
      "REF: `` and then i 'll kind of be like — ( <unk> ! ) — and then they 're like , `` '' whoa ! '' '' ( laughter ) i 'm sure you 're trying to figure out , `` well , how does this thing work ? '' ''\n",
      "OUT: and then , it 's like this — ( laughter ) — i 'm sure you 're trying to figure out how to figure out how this thing works .\n"
     ]
    }
   ],
   "source": [
    "translate_sample(model1, data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to generate translations from user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translate_input (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function translate_input(model)\n",
    "    v = model.srcvocab\n",
    "    src = [ get(v.w2i, w, v.unk) for w in v.tokenizer(readline()) ]'\n",
    "    out = model(src)\n",
    "    println(\"SRC: \", int2str(src,model.srcvocab))\n",
    "    println(\"OUT: \", int2str(out,model.tgtvocab))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate translations for user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate_input(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition\n",
    "\n",
    "The reference model `pretrained` has 16.2 bleu. By playing with the optimization algorithm\n",
    "and hyperparameters, using per-sentence loss, and (most importantly) splitting the Turkish\n",
    "words I was able to push the performance to 21.0 bleu. I will give extra credit to groups\n",
    "that can exceed 21.0 bleu in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
